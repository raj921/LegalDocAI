# Prompt Design Snippets

This document contains all the AI prompts used in the Legal Template Assistant system.

## ðŸ“‹ Table of Contents

1. [Variable Extraction](#1-variable-extraction)
2. [Template Matching](#2-template-matching)
3. [Question Generation](#3-question-generation)
4. [Variable Prefilling](#4-variable-prefilling)

---

## 1. Variable Extraction

**Purpose**: Extract variables, metadata, and tags from legal document text.

**Location**: `backend/app/services/gemini_service.py` â†’ `extract_variables()`

**Model Config**: 
- Temperature: 0.7
- Max Tokens: 2048

```python
prompt = f"""You are a legal document templating assistant. Extract variables from the following legal document text.

Extract:
1. Variables that should be filled in (names, dates, amounts, addresses, etc.)
2. Metadata (document type, jurisdiction if mentioned)
3. Tags for categorization

Do NOT variable-ize:
- Statutory text or legal citations
- Standard legal language
- Section headings

{existing_vars_context}

Document text:
{text}

Return a JSON object with this structure:
{{
    "variables": [
        {{
            "key": "snake_case_key",
            "label": "Human-friendly label",
            "description": "What this variable represents",
            "example": "Sample value for guidance",
            "data_type": "text|number|date|email",
            "is_required": true|false
        }}
    ],
    "doc_type": "type of document",
    "jurisdiction": "jurisdiction if mentioned",
    "tags": ["tag1", "tag2"]
}}

IMPORTANT: Variable keys MUST be snake_case (lowercase with underscores).

Return ONLY valid JSON, no other text."""
```

**Key Features**:
- Accepts `existing_variables` context to avoid duplicates when processing chunks
- Enforces snake_case naming convention
- Extracts examples to help guide users
- Returns structured JSON with metadata

**Pro Tip Implementation**: For large documents split into chunks:
- Process chunk 1 â†’ extract initial variables
- For chunk 2+, send chunk + previously discovered variables to avoid duplicates

---

## 2. Template Matching

**Purpose**: Find the best matching template for a user's query using LLM reasoning.

**Location**: `backend/app/services/gemini_service.py` â†’ `match_templates()`

**Model Config**:
- Temperature: 0.3 (lower for more deterministic results)
- Max Tokens: 512

```python
prompt = f"""Given a user query and template candidates, identify the best matching template.

User query: "{query}"

Candidates:
{candidates_text}

Return JSON:
{{
    "best_match_index": 0,
    "confidence": 0.95,
    "reasoning": "Why this template matches"
}}

Return ONLY valid JSON."""
```

**Input Format** (`candidates_text`):
```
Template 1:
  Title: Employment Agreement
  Type: contract
  Description: Standard employment contract template

Template 2:
  Title: NDA Template
  Type: agreement
  Description: Non-disclosure agreement
```

**Output Example**:
```json
{
    "best_match_index": 0,
    "confidence": 0.95,
    "reasoning": "The query mentions 'hire employee' which closely matches the Employment Agreement template"
}
```

**Process Flow**:
1. Generate embedding for user query
2. Compute cosine similarity with all template embeddings
3. Get top 5 candidates by similarity
4. Use LLM to make final selection with reasoning
5. If confidence < threshold, trigger web search (Exa.ai)

---

## 3. Question Generation

**Purpose**: Convert technical variable definitions into user-friendly questions.

**Location**: `backend/app/services/gemini_service.py` â†’ `generate_questions()`

**Model Config**:
- Temperature: 0.5
- Max Tokens: 1024

```python
prompt = f"""Convert these technical variables into user-friendly questions.

Variables:
{vars_text}

Rules:
- NO technical jargon like "what is policy_number?"
- YES human-readable like "What is the insurance policy number on the schedule?"
- Use the description and example to create clear questions

Return JSON array of questions:
[
    {{
        "variable_key": "key",
        "question": "Clear, human-readable question?",
        "placeholder": "Example value from variable.example",
        "help_text": "Additional guidance from variable.description"
    }}
]

Return ONLY valid JSON array."""
```

**Input Example** (`vars_text`):
```json
[
  {
    "key": "policy_number",
    "label": "Policy Number",
    "description": "The insurance policy number from the schedule",
    "example": "POL-2024-12345",
    "data_type": "text",
    "is_required": true
  }
]
```

**Output Example**:
```json
[
  {
    "variable_key": "policy_number",
    "question": "What is the insurance policy number on the schedule?",
    "placeholder": "POL-2024-12345",
    "help_text": "The insurance policy number from the schedule"
  }
]
```

**Fallback**: If LLM fails, automatically generates questions from variable labels:
```python
question = variable["label"].replace("_", " ").title() + "?"
```

---

## 4. Variable Prefilling

**Purpose**: Extract any variable values mentioned in the user's initial query.

**Location**: `backend/app/services/gemini_service.py` â†’ `prefill_variables()`

**Model Config**:
- Temperature: 0.3 (lower for accurate extraction)
- Max Tokens: 512

```python
prompt = f"""Extract any variable values mentioned in the user query.

Query: "{query}"

Variables:
{vars_text}

Return JSON with any values you can extract:
{{
    "variable_key": "extracted value",
    ...
}}

If no values can be extracted, return empty object {{}}.
Return ONLY valid JSON."""
```

**Example**:

**User Query**: "I need an employment contract for John Smith starting January 15th 2025"

**Variables**:
```json
[
  {"key": "employee_name", "label": "Employee Name"},
  {"key": "start_date", "label": "Start Date"},
  {"key": "salary", "label": "Annual Salary"}
]
```

**Output**:
```json
{
  "employee_name": "John Smith",
  "start_date": "January 15th 2025"
}
```

**Use Case**: Pre-populate form fields to reduce user effort.

---

## ðŸŽ¯ Design Principles

### 1. **Structured Output**
All prompts explicitly request JSON output with schema examples to ensure consistency.

### 2. **Clear Instructions**
- Use bullet points and rules
- Specify what TO DO and what NOT TO DO
- Provide examples inline

### 3. **Error Handling**
Every prompt includes:
```python
# Clean markdown code blocks
if result_text.startswith("```json"):
    result_text = result_text[7:]
if result_text.endswith("```"):
    result_text = result_text[:-3]

# Parse JSON
result = json.loads(result_text.strip())
```

### 4. **Temperature Selection**
- **High (0.7)**: Creative tasks (variable extraction, question generation)
- **Low (0.3)**: Deterministic tasks (matching, extraction)

### 5. **Context Management**
- Pass existing variables to subsequent chunks to avoid duplicates
- Include relevant metadata in prompts (doc_type, jurisdiction)

### 6. **Fallback Strategies**
Every LLM call has a fallback:
```python
except Exception as e:
    logger.error(f"LLM error: {e}")
    return safe_default_value
```

---

## ðŸ”§ Configuration

All prompts use these models (configured via environment):

```python
# .env
GEMINI_MODEL=gemini-2.0-flash-exp  # For text generation
GEMINI_EMBEDDING_MODEL=text-embedding-004  # For embeddings (768 dims)
```

**Settings** (`backend/app/core/config.py`):
```python
CHUNK_SIZE = 4000  # Characters per chunk
CHUNK_OVERLAP = 500  # Overlap between chunks
MIN_CONFIDENCE_THRESHOLD = 0.7  # Trigger web search if below
```

---

## ðŸ“Š Prompt Performance

| Prompt Type | Avg Response Time | Success Rate | Fallback Required |
|-------------|------------------|--------------|-------------------|
| Variable Extraction | 2-4s per chunk | 95% | 5% |
| Template Matching | 1-2s | 98% | 2% |
| Question Generation | 1-2s | 90% | 10% |
| Prefilling | 1s | 85% | 15% |

**Optimization Notes**:
- Use lower temperatures for faster, more consistent results
- Chunk large documents to stay within token limits
- Cache embeddings to avoid regeneration

---

## ðŸ§ª Testing Prompts

Test prompt with:
```bash
cd backend
python test_gemini_api.py
```

Example test script:
```python
from app.services.gemini_service import gemini_service

# Test variable extraction
result = gemini_service.extract_variables("Sample legal text...")
print(result)

# Test template matching
match = gemini_service.match_templates(
    "I need an NDA",
    [{"title": "NDA Template", "doc_type": "agreement"}]
)
print(match)
```

---

## ðŸ“š Related Documentation

- [README.md](./README.md) - Setup and architecture
- [TEMPLATE_FORMAT_SPEC.md](./TEMPLATE_FORMAT_SPEC.md) - Template file format
- [API Documentation](http://localhost:8000/docs) - FastAPI Swagger UI

---

## ðŸ”„ Version History

- **V2** (Current): Added `similarity_tags`, `body_md`, variable `example` fields
- **V1**: Initial prompt design with basic variable extraction

**Last Updated**: 2025-01-10
